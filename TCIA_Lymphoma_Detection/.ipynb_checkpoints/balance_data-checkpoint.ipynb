{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing Training Data\n",
    "The data directory generated by our Gen_Data script is highly biased towards negative images. Let's fix that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from cv2 import imread\n",
    "from matplotlib.pyplot import imshow\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this in terminal:\n",
    ">cd ~/sara_m/data<br/>\n",
    ">mkdir ct_balanced<br/>\n",
    ">cp -R ct_data/. ct_balanced<br/>\n",
    ">mv  -v ./valid/1/* ./train/1<br/>\n",
    ">mv  -v ./valid/0/* ./train/0<br/>\n",
    "\n",
    "These commands, when run sequentially, creates a new directory with identical contents to our original data (copying prevents accidental destruction of data), and move all our image data into the train/ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/ct_data/valid/1\n",
      "98\n",
      "./data/ct_data/valid/0\n",
      "12301\n",
      "./data/ct_data/train/1\n",
      "122\n",
      "./data/ct_data/train/0\n",
      "16732\n"
     ]
    }
   ],
   "source": [
    "data_path = './data/ct_balanced/'\n",
    "files_net = []\n",
    "for path, dirs, files in os.walk(data_path):\n",
    "    if len(dirs)==0:\n",
    "        print(path)\n",
    "        print(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Unbalanced Data\n",
    "Right now there are nearly 100x more negative images than positive - this makes sense, as we generated our dataset from 3D volumetric slices from patient CT scans, and tumors (even in a relatively sick patient) should only appear in a small subset of those slices.\n",
    "\n",
    "We could build a pipeline to augment our negative images exclusively in an attempt to decrease this discrepancy; however, augmentation for a factor of 100x datapoints is asking a lot, even for the state of the art. Better to randomly downsample our negative datapoints and augment the entire image dataset at training. The next two cells do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current ratio: 220:660 or 0.3333333333333333\n",
      "We want a ratio of around 1:3, or .33\n"
     ]
    }
   ],
   "source": [
    "#First: randomly remove files from train/0 to bring positive:negative ratio up to 1:3\n",
    "pos_path = '/home/ubuntu/sara_m/data/ct_balanced/train/1'\n",
    "neg_path = '/home/ubuntu/sara_m/data/ct_balanced/train/0'\n",
    "pos_files = os.listdir(pos_path)\n",
    "n_pos = len(pos_files)\n",
    "neg_files = os.listdir(neg_path)\n",
    "n_neg = len(neg_files)\n",
    "\n",
    "target_ratio = 1/3\n",
    "chosen = set([os.path.join(neg_path,f) for f in np.random.choice(neg_files,int(((1/target_ratio)*n_pos)), replace=False)])\n",
    "to_delete = [os.path.join(neg_path,f) for f in neg_files if os.path.join(neg_path,f) not in chosen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25847/25847 [00:01<00:00, 24498.95it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in tqdm(to_delete):\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check\n",
    "Let's view the distribution of our data now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/ct_balanced/valid/1\n",
      "0\n",
      "./data/ct_balanced/valid/0\n",
      "0\n",
      "./data/ct_balanced/train/1\n",
      "220\n",
      "./data/ct_balanced/train/0\n",
      "660\n"
     ]
    }
   ],
   "source": [
    "#list data again:\n",
    "files_net = []\n",
    "for path, dirs, files in os.walk(data_path):\n",
    "    if len(dirs)==0:\n",
    "        print(path)\n",
    "        print(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems good. This is a fairly small dataset after all is said and done; however, augmentation should address that somewhat, and remember, we still have the full dataset in our other directory to train the model on further after-the-fact.\n",
    "\n",
    "### Moving Things Back\n",
    "Now let's move ~ 25% of the data back to the valid/ directory. We'll use this to assess the performance of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 28933.49it/s]\n",
      "100%|██████████| 55/55 [00:00<00:00, 32628.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/ct_balanced/train/0 has 660; move 165\n",
      "Moving!\n",
      "./data/ct_balanced/train/1 has 220; move 55\n",
      "Moving!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in list(range(2)):\n",
    "    d = f'./data/ct_balanced/train/{i}'\n",
    "    dest = f'./data/ct_balanced/valid/{i}'\n",
    "    \n",
    "    files = os.listdir(d) \n",
    "    n_files = len(files)\n",
    "    n_move = int(.25*n_files)\n",
    "    print(f'{d} has {n_files}; move {n_move}')\n",
    "    to_move = [os.path.join(d,f) for f in np.random.choice(files, n_move, replace=False)]\n",
    "    print('Moving!')\n",
    "    for f in tqdm(to_move):\n",
    "        shutil.move(f, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check\n",
    "Let's view the distribution of data one last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/ct_balanced/valid/1\n",
      "55\n",
      "./data/ct_balanced/valid/0\n",
      "165\n",
      "./data/ct_balanced/train/1\n",
      "165\n",
      "./data/ct_balanced/train/0\n",
      "495\n"
     ]
    }
   ],
   "source": [
    "#list data one last time:\n",
    "files_net = []\n",
    "for path, dirs, files in os.walk(data_path):\n",
    "    if len(dirs)==0:\n",
    "        print(path)\n",
    "        print(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "Let's move on to training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_science] *",
   "language": "python",
   "name": "conda-env-data_science-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
